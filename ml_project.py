# -*- coding: utf-8 -*-
"""ML_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m6YEnPaJy5oIcEnDF3r2O8kL3htNKNOl

# **Marketing Campaign for Banking Products**

### ***1. Import the datasets and libraries, check datatype,statistical summary, shape, null values etc***

**1.1 Importing the required libraries for EDA** 

---
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import sympy as sp
import seaborn as sns                #visualization 
import matplotlib.pyplot as plt     #visualization 
# %matplotlib inline
plt.rcParams["figure.figsize"] =(10,6)

from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier

"""**1.2 Importing the Datasets**

---
"""

from google.colab import files
uploaded = files.upload()

import io
!pip install -q xlrd 
#df=pd.read_excel(io.bytesIO(uploaded['Bank_personal_loan _modelling.xlsx']))
model=pd.read_csv('Bank_Personal_Loan_Modelling.csv')

model.head(10)

model.tail(10)

"""**1.3 Checking the types of data and basic summary stats**

---
"""

model.info()

model.describe()

model.median()

"""### ***2. Check if you need to clean the data for any of the variables***

***2.1 Cleaning of Data***

---
"""

model['Experience'].replace( to_replace= -1,value = np.nan,inplace = True )
model['Experience'].replace( to_replace= -2,value = np.nan,inplace = True )
model['Experience'].replace( to_replace= -3,value = np.nan,inplace = True )
model['Experience'].fillna(model['Experience'].median(),inplace=True)

"""### ***3. Study the data distribution in each attribute and target variable, share your findings.***

**3.1  Number of unique in each column**

---
"""

for column in model.columns:
   print(f' Unique values in {column} is {len(model[column].unique())}')

sns.pairplot(model.iloc[:,4:])

"""**3.2  Number of people with zero mortgage**

---
"""

print('Number of people with zero mortgage:',len(model[model['Mortgage']==0]))

"""**3.3 Number of people with zero credit card spending per month**

---
"""

print('Number of people with zero Credit Card spending per month:', len(model[model['CCAvg']==0]))

"""**3.4 Value counts of all categorical columns.**

---
"""

categorical_columns = ['CCAvg', 'Family', 'Education', 'Securities Account', 'CD Account', 'Online', 'CreditCard', 'Personal Loan']

for column in categorical_columns:
    print(f'Value counts for {column} column:')
    print(model[column].value_counts())
    print()

"""**3.5 Univariate and Bivariate analysis**


---

*3.5.1 Univiariate Analysis*

---



---
"""

sns.distplot(model.Age)

sns.distplot(model.CCAvg)

sns.distplot(model.Income)

plt.hist(model.Mortgage)

sns.countplot(model.Family)

sns.countplot(model.Education)

"""*3.5.2 Bivariate analysis*

---



---
"""

sns.boxplot(x='Education',y='Income',hue='Personal Loan',data=model)

sns.countplot(x="Securities Account", data=model,hue="Personal Loan")

Data=pd.DataFrame(model['Personal Loan'].value_counts()).reset_index()
Data.columns=['Labels','Personal Loan']
fig1,ax1=plt.subplots(figsize=(8,6))
explode=(0,0.25)
ax1.pie(Data['Personal Loan'],explode=explode,autopct='%1.2f%%',shadow=True,startangle=110)
ax1.axis('equal')
plt.title('Personal Loan %')
plt.show()

corr = model.corr()

sns.heatmap(corr, linewidths=1)

"""### ***4. Apply necessary transformations for the feature variables***"""

model.drop(['ID','ZIP Code'], axis=1, inplace=True)

from sklearn.preprocessing import MinMaxScaler

columns_to_scale = ['Age', 'Family', 'Income', 'Education', 'CCAvg',  'Mortgage']

for column in columns_to_scale:
    scaler = MinMaxScaler()
    model[column] = scaler.fit_transform(model[column].values.reshape(-1, 1))
    
model.head()

X = model.drop(['Personal Loan'], axis=1)
y = model['Personal Loan'].values

X.shape, y.shape

"""### ***5. Normalise data and split the data into training and test set in the ratio of 70:30 respectively***"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify= y,random_state=17)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

class_names = ['wont take loan', 'take loan']

"""### ***6. Use the Logistic Regression model to predict the likelihood of a customer buying personal loans.***

### ***7. Printing all the metrics related for evaluating the model performance***

**LOGISTIC REGGRESION**

---
"""

log_reg = LogisticRegression(C=1.0, max_iter=200)
log_reg.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, plot_confusion_matrix, plot_roc_curve, roc_auc_score
from sklearn.metrics import plot_precision_recall_curve

print('Logistic Regression Results: ')

train_score = log_reg.score(X_train, y_train)
print('Training Accuracy:', train_score.round(2))
test_score = log_reg.score(X_test, y_test)
print('Testing Accuracy:', test_score.round(2))

y_pred_logi = log_reg.predict(X_test)

precision_logi = precision_score(y_test, y_pred_logi, labels=class_names).round(2)
print('Precision:', precision_logi)
recall_logi = recall_score(y_test, y_pred_logi).round(2)
print('Recall:', recall_logi)

plot_confusion_matrix(log_reg, X_test, y_test, display_labels=class_names, values_format='d')
plt.title('Confusion Matrix for Logistic Regression')

plot_roc_curve(log_reg, X_test, y_test)
plt.title('ROC Curve for Logistic Regression')

plot_precision_recall_curve(log_reg, X_test, y_test)
plt.title('Precision-Recall Curve for Logistic Regression')

"""### ***8. Build various other classification algorithms and compare their performance***

**SUPPORT VECTOR MACHINE (SVM)**

---
"""

from sklearn import svm
import sklearn.metrics as metrics
from sklearn.model_selection import train_test_split, GridSearchCV

sv= svm.SVC(kernel='linear',C=5)
sv.fit(X_train,y_train)

pred_test = sv.predict(X_test)
pred_train = sv.predict(X_train)

acc_Xtest= accuracy_score(y_test,pred_test)
acc_Xtrain= accuracy_score(y_train,pred_train)
print('Accuracy on the X_train is : {:.4f}' .format(acc_Xtrain))
print('Accuracy on the X_test is : {:.4f}' .format(acc_Xtest))

cm = metrics.confusion_matrix(y_test,pred_test)
print(cm)

print(classification_report(y_test,pred_test))

plot_confusion_matrix(sv, X_test, y_test, display_labels=class_names, values_format='d')
plt.title('Confusion Matrix for SVM')

print('Recall: ',recall_score(y_test,pred_test))
print('Precision: ',precision_score(y_test,pred_test))
print('F1 Score: ',f1_score(y_test,pred_test))
print('roc_auc_score: ',roc_auc_score(y_test,pred_test))

plot_roc_curve(sv, X_test, y_test)
plt.title('ROC Curve for SVM')

plot_precision_recall_curve(sv, X_test, y_test)
plt.title('Precision-Recall SVM')

"""**RANDOM FOREST CLASSIFIER**

---
"""

rac = RandomForestClassifier(n_estimators=300, max_depth=7,n_jobs=-1 )
rac.fit(X_train, y_train)

print('Random Forest Classifier Results: ')

train_score = rac.score(X_train, y_train)
print('Training Accuracy:', train_score.round(2))
test_score = rac.score(X_test, y_test)
print('Testing Accuracy:', test_score.round(2))

y_pred_rf = rac.predict(X_test)

precision_rf = precision_score(y_test, y_pred_rf, labels=class_names).round(2)
print('Precision:', precision_rf)
recall_rf = recall_score(y_test, y_pred_rf).round(2)
print('Recall:', recall_rf)

plot_confusion_matrix(rac, X_test, y_test, display_labels=class_names, values_format='d')
plt.title('Confusion Matrix for Random Forest Classifier')

plot_roc_curve(rac, X_test, y_test)
plt.title('ROC Curve for Random Forest Classifier')

plot_precision_recall_curve(rac, X_test, y_test)
plt.title('Precision-Recall Random Forest Classifier')

"""**DECISION TREE**

---
"""

from sklearn.tree import DecisionTreeClassifier
import sklearn.metrics as metrics

from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree

#feature Scaling
classifier= DecisionTreeClassifier(random_state=0, max_depth=8)
classifier.fit(X_train,y_train)

#predicting on training and test data
y_train_pred = classifier.predict(X_train)
y_test_pred= classifier.predict(X_test)

pred_test = classifier.predict(X_test)
pred_train = classifier.predict(X_train)

cm = metrics.confusion_matrix(y_test,pred_test)
print(cm)

acc_Xtest= accuracy_score(y_test,pred_test)
acc_Xtrain= accuracy_score(y_train,pred_train)
print('Accuracy on the X_train is : {:.4f}' .format(acc_Xtrain))
print('Accuracy on the X_test is : {:.4f}' .format(acc_Xtest))

plt.figure(figsize=(5,5))
sns.heatmap(cm,annot=True,fmt='.2f',linewidth=0.5,square=True,cmap='Blues_r')
plt.xlabel('Predicted value')
plt.ylabel('Actual values')
all_sampl_title= 'accuracy score: {0}' .format(acc_Xtest*100)
plt.title(all_sampl_title);

print('Recall: ',recall_score(y_test,pred_test))
print('Precision: ',precision_score(y_test,pred_test))
print('F1 Score: ',f1_score(y_test,pred_test))
print('ROC_AUC Score: ',roc_auc_score(y_test,pred_test))

from sklearn.metrics import classification_report

print(classification_report(y_test,pred_test))

"""# ***9. Give a business understanding of your model***


### **The classification goal is to predict the likelihood of a liability customer buying personal loans.**

***Content:***

1. Intro
2. Problem statement
3. Python libraries
4. Independent and dependent variable
5. Classification Algorithms Used
6. Interpretation
7. Conclusion


###1. INTRODUCTION

This is about a bank which has a growing customer base. The bank wants to increase borrowers (asset customers) base to bring in more loan business and earn more through the interest on loans. So , the bank wants to convert the liability based customers to personal loan customers (while retaining them as depositors).

 A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. Now the department wants us to build a model that will help the bank identify the potential customers who have a higher probability of purchasing the loan. 
 
 This will increase the success ratio while at the same time also reduces the cost of the campaign.


###2. PROBLEM STATEMENT

The main aim of this model is to predicting the way to explore ways of converting its liability customers (depositors) to personal loan customers (while retaining them as depositors) using some continous and discrete variable data. 

Finding relationship or dependency of Personal Loan on attrubutes like Income, Age ,Experience etc.

###3. PYTHON LIBRARIES

- Numpy

- Pandas

- Matplotlib

- Seaborn

- Scikit-Learn

###4. Independent and dependent variable

* Independent variable

Independent variable (X) = Feature variable = Predictor variable

The following are the independent variable:-

1. ID : Customer ID
2. Age : Customer's age in completed years
3. Experience : years of professional experience
4. Income : Annual income of the customer ( 000)
5. ZIPCode:HomeAddressZIPcode.
6. Family:Familysizeofthecustomer
7. CCAvgAvg.:spendingoncreditcardspermonth( 000)
8. Education : Education Level. 1: Undergrad; 2: Graduate; 3: Advanced/Professional
9. Mortgage : Value of house mortgage if any. ($000)
10. Securities Account : Does the customer have a securities account with the bank?
11. CD Account : Does the customer have a certificate of deposit (CD) account with the bank?
12. Online : Does the customer use internet banking facilities?
13. CreditCard :Does the customer use a credit card issued by UniversalBank?


* Dependent variable

Dependent variable (y) = Target variable = Response variable

The following is the dependent variable:-

Personal Loan :Did this customer accept the personal loan offered in the last campaign?


###5. CLASSIFICATION ALGORITHMS USED

Logistic Regression

Support Vector Machine (SVM)

Random Forest Classifier

Decision Tree

###6. INTERPRETATION

* Logistic Regression

Training Accuracy: 0.95

Testing Accuracy: 0.96

Precision: 0.89

Recall: 0.63

* Support Vector Machine (SVM)

Training Accuracy : 0.9523

Testing Accuracy : 0.9540

Precision: 0.8761904761904762

Recall: 0.6216216216216216

* Random Forest Classifier

Training Accuracy: 0.99

Testing Accuracy: 0.99

Precision: 0.98

Recall: 0.92

* Decision Tree

Training Accuracy : 0.9980

Testing Accuracy : 0.9767

Precision: 0.9051094890510949

Recall: 0.8493150684931506



###7. CONCLUSION

We have implemented 5 classifier models.

From the above implemented classifiers RandomForestClassifier is the best model to predict ikelihood of a liability customer buying personal loans with 99% training and testing accuracy.
"""